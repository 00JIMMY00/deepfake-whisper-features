{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0cf500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/deepfake/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
    "# Cell 1 - Core imports and logging setup\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Union\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import yaml\n",
    "\n",
    "from src import commons\n",
    "from src.datasets.base_dataset import APPLY_NORMALIZATION, apply_preprocessing\n",
    "from src.models import models\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(\"inference_notebook\")\n",
    "if not LOGGER.handlers:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ea9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
    "# Cell 2 - Paths, device configuration, and deterministic setup\n",
    "DEFAULT_CONFIG_PATH = Path(\"/Users/ahmedgamal/Downloads/deepfake-whisper-features-1/configs/training/whisper_specrnet.yaml\")\n",
    "DEFAULT_WEIGHTS_PATH = Path(\"/Users/ahmedgamal/Downloads/whisper_specrnet/weights.pth\")\n",
    "# DEFAULT_WEIGHTS_PATH = Path(\"trained_models/whisper_specrnet/best_model.pth\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path, None]) -> Optional[Path]:\n",
    "    \"\"\"Safely convert incoming path-like objects into `Path` instances.\"\"\"\n",
    "\n",
    "    if path is None:\n",
    "        return None\n",
    "\n",
    "    path_obj = Path(path)\n",
    "    if str(path_obj).strip() == \"\":\n",
    "        return None\n",
    "    return path_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4dba497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
    "# Cell 3 - Configuration loading utilities\n",
    "def load_inference_config(config_path: Union[str, Path]) -> Dict[str, Any]:\n",
    "    \"\"\"Load YAML configuration that defines the inference model.\"\"\"\n",
    "\n",
    "    config_path = Path(config_path)\n",
    "    if not config_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Configuration file not found at '{config_path.resolve()}'\"\n",
    "        )\n",
    "\n",
    "    LOGGER.info(\"Loading inference configuration from %s\", config_path)\n",
    "    with config_path.open(\"r\", encoding=\"utf-8\") as file:\n",
    "        config: Dict[str, Any] = yaml.safe_load(file)\n",
    "\n",
    "    seed = config.get(\"data\", {}).get(\"seed\", 42)\n",
    "    commons.set_seed(seed)\n",
    "    LOGGER.info(\"Random seed fixed at %s\", seed)\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e15618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inference_notebook:Loading inference configuration from /Users/ahmedgamal/Downloads/deepfake-whisper-features-1/configs/training/whisper_specrnet.yaml\n",
      "INFO:inference_notebook:Random seed fixed at 42\n",
      "INFO:inference_notebook:Creating model 'whisper_specrnet' on cpu\n",
      "/Users/ahmedgamal/Downloads/deepfake-whisper-features-1/src/models/whisper_specrnet.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(WHISPER_MODEL_WEIGHTS_PATH)\n",
      "/opt/miniconda3/envs/deepfake/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:inference_notebook:Loading model weights from /Users/ahmedgamal/Downloads/whisper_specrnet/weights.pth\n",
      "/var/folders/7v/x7s3qxj90bq09ym9kd6sgt6m0000gn/T/ipykernel_15878/3633270572.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_file, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
    "# Cell 4 - Model creation and weight restoration\n",
    "def build_model(\n",
    "    model_config: Dict[str, Any],\n",
    "    weights_path: Optional[Union[str, Path]] = None,\n",
    "    device: str = DEVICE,\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"Instantiate the neural network and optionally restore trained weights.\"\"\"\n",
    "\n",
    "    model_name = model_config[\"name\"]\n",
    "    model_parameters = model_config.get(\"parameters\", {})\n",
    "\n",
    "    LOGGER.info(\"Creating model '%s' on %s\", model_name, device)\n",
    "    model = models.get_model(model_name=model_name, config=model_parameters, device=device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    weights_file = _resolve_path(weights_path)\n",
    "    if weights_file and weights_file.exists():\n",
    "        LOGGER.info(\"Loading model weights from %s\", weights_file)\n",
    "        state_dict = torch.load(weights_file, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        LOGGER.warning(\n",
    "            \"Weights file not found. Inference will use randomly initialized weights.\"\n",
    "        )\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "INFERENCE_CONFIG = load_inference_config(DEFAULT_CONFIG_PATH)\n",
    "INFERENCE_MODEL = build_model(\n",
    "    model_config=INFERENCE_CONFIG[\"model\"],\n",
    "    weights_path=INFERENCE_CONFIG.get(\"checkpoint\", {}).get(\"path\")\n",
    "    or DEFAULT_WEIGHTS_PATH,\n",
    "    device=DEVICE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e60c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
    "# Cell 5 - Audio loading and preprocessing helpers\n",
    "def load_waveform(audio_path: Union[str, Path]) -> torch.Tensor:\n",
    "    \"\"\"Load a waveform from disk and apply dataset-aligned preprocessing.\"\"\"\n",
    "\n",
    "    audio_path = Path(audio_path)\n",
    "    if not audio_path.exists():\n",
    "        raise FileNotFoundError(f\"Audio file not found at '{audio_path.resolve()}'\")\n",
    "\n",
    "    waveform, sample_rate = torchaudio.load(\n",
    "        str(audio_path), normalize=APPLY_NORMALIZATION\n",
    "    )\n",
    "    waveform, _ = apply_preprocessing(waveform, sample_rate)\n",
    "\n",
    "    return waveform.float()\n",
    "\n",
    "\n",
    "def prepare_batch(waveform: torch.Tensor, device: str = DEVICE) -> torch.Tensor:\n",
    "    \"\"\"Prepare a batch tensor compatible with the model's expected input.\"\"\"\n",
    "\n",
    "    if waveform.dim() == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    return waveform.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
    "# Cell 6 - Inference helpers and probability conversion\n",
    "def predict_probability(\n",
    "    model: torch.nn.Module,\n",
    "    batch_waveform: torch.Tensor,\n",
    ") -> float:\n",
    "    \"\"\"Run the forward pass and return the bonafide (real) probability.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch_waveform).squeeze(1)\n",
    "        probability = torch.sigmoid(logits)\n",
    "\n",
    "    return float(probability.detach().cpu().item())\n",
    "\n",
    "\n",
    "def probability_to_label(probability: float, threshold: float = 0.5) -> str:\n",
    "    \"\"\"Convert bonafide probability into a human-readable label.\"\"\"\n",
    "\n",
    "    return \"real\" if probability >= threshold else \"fake\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ede97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
    "# Cell 7 - Public API to classify external audio files\n",
    "def classify_audio_file(\n",
    "    audio_path: Union[str, Path],\n",
    "    *,\n",
    "    model: torch.nn.Module = INFERENCE_MODEL,\n",
    "    device: str = DEVICE,\n",
    "    decision_threshold: float = 0.5,\n",
    ") -> str:\n",
    "    \"\"\"Classify an input `.wav` file as real (bonafide) or fake (spoof).\"\"\"\n",
    "\n",
    "    waveform = load_waveform(audio_path)\n",
    "    batch = prepare_batch(waveform, device=device)\n",
    "    score = predict_probability(model, batch)\n",
    "    label = probability_to_label(score, threshold=decision_threshold)\n",
    "\n",
    "    LOGGER.info(\n",
    "        \"File '%s' classified as %s with probability %.3f\", audio_path, label, score\n",
    "    )\n",
    "    return label\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"classify_audio_file\",\n",
    "    \"load_inference_config\",\n",
    "    \"build_model\",\n",
    "    \"load_waveform\",\n",
    "    \"predict_probability\",\n",
    "    \"probability_to_label\",\n",
    "    \"prepare_batch\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30648560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e48f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
